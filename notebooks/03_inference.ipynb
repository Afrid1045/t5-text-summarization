{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Inference with T5\n",
    "## Introduction\n",
    "In this notebook, we will use the pre-trained T5 model to summarize real-world text data.\n",
    "We will:\n",
    "- Load the model and tokenizer.\n",
    "- Test summarization on various types of long text.\n",
    "- Experiment with different decoding strategies to improve results.\n",
    "\n",
    "## Step 1: Load Model & Tokenizer\n",
    "Before performing inference, we need to load the pre-trained T5 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Model & Tokenizer\n",
    "We start by loading the pre-trained **T5-small** model and its tokenizer. \n",
    "\n",
    "- `T5Tokenizer.from_pretrained(\"t5-small\")` → Loads the tokenizer.\n",
    "- `T5ForConditionalGeneration.from_pretrained(\"t5-small\")` → Loads the summarization model.\n",
    "- We use **PyTorch tensors** (`return_tensors=\"pt\"`) for model compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 't5-small' loaded successfully.\n",
      "Running on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Print Model Info\n",
    "print(f\"Model '{model_name}' loaded successfully.\")\n",
    "print(f\"Running on: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"summarize: Green Arrow is a superhero who appears in American comic books published by DC Comics. Created by Mort Weisinger and designed by George Papp, he first appeared in More Fun Comics No. 73 on September 19, 1941 (cover dated November 1941), the same issue that debuted Aquaman. His real name is Oliver Jonas Queen, a wealthy businessman, owner of Queen Industries, and a well-known celebrity in Star City. He uses this position to hide the fact that he is Green Arrow.[1] Partly inspired by Robin Hood, Green Arrow is an archer who uses his skills to fight crime in his home cities of Star City and Seattle, as well as alongside his fellow superheroes as a member of the Justice League. The world's greatest archer, as well as a competent swordsman and martial artist, Green Arrow deploys a range of trick arrows (in contemporary times, they are referred as \"specialty arrows\"[2]) with various special functions, such as glue, explosive-tipped, grappling hook, flash grenade, tear gas, and even kryptonite arrows for use in a range of special situations.\n",
    "\n",
    "Green Arrow enjoyed moderate success in his early years, becoming the cover feature of More Fun, as well as having occasional appearances in other comics. Throughout his first twenty-five years, however, the character never enjoyed greater popularity. In the late 1960s, writer Denny O'Neil, inspired by the character's dramatic visual redesign by Neal Adams, chose to have him lose his fortune, giving him the then-unique role of a streetwise crusader for the working class and the disadvantaged. In 1970, he was paired with a more law and order-oriented hero, Green Lantern, in a ground-breaking, socially conscious comic book series.[3] Since then, he has been popular among comic book fans and most writers have taken an urban, gritty approach to the character. Oliver Queen was killed off in the 1990s and replaced by a new character, Oliver's son Connor Hawke. Connor, however, proved a less popular character, and the original Oliver Queen character was resurrected in the 2001 \"Quiver\" storyline, by writer Kevin Smith. In the 2000s, the character has been featured in bigger storylines focusing on Green Arrow and Black Canary, such as the DC event The Green Arrow/Black Canary Wedding and the high-profile Justice League: Cry for Justice storyline, prior to the character's relaunch alongside most of DC's properties in 2011.\n",
    "\n",
    "Green Arrow was not initially a well-known character outside of comic book fandom: He had appeared in a single episode of the animated series Super Friends in 1973. In the 2000s, the character appeared in a number of DC television properties, including the animated series Justice League Unlimited, Young Justice, The Batman and Batman: The Brave and the Bold, and several DC Universe Animated Original Movies. In live action, he appeared in the series Smallville, played by actor Justin Hartley, and became a core cast member. In 2012, the live action series Arrow debuted on The CW, in which the title character was portrayed by Stephen Amell, and launching several spin-off series, becoming the starting point for a shared television franchise called the Arrowverse.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[21603,    10,  1862, 25810,    19,     3,     9, 23586,   113,  3475,\n",
      "            16,   797,  7967,  1335,  1790,    57,  5795, 15175,     7,     5,\n",
      "          6357,    26,    57, 19729,   101,  4890,    49,    11,   876,    57,\n",
      "          3080,   276,  3096,     6,     3,    88,   166,  4283,    16,  1537,\n",
      "          9259, 15175,     7,   465,     5,     3,  4552,    30,  1600, 12370,\n",
      "         24822,    41,  9817,     3, 14134,  1671, 24822,   201,     8,   337,\n",
      "           962,    24,    20, 29261, 11154,   348,     5,   978,   490,   564,\n",
      "            19, 15865,  8178,     9,     7,  5286,     6,     3,     9, 18407,\n",
      "           268,   348,     6,  2527,    13,  5286, 18080,     6,    11,     3,\n",
      "             9,   168,    18,  5661, 17086,    16,  2042,   896,     5,   216,\n",
      "          2284,    48,  1102,    12,  7387,     8,   685,    24,     3,    88,\n",
      "            19,  1862, 25810,     5,  6306,   536,   908,  2733,   120,  3555,\n",
      "            57, 14059, 19804,     6,  1862, 25810,    19,    46, 11508,    49,\n",
      "           113,  2284,   112,  1098,    12,  2870,  5447,    16,   112,   234,\n",
      "          3119,    13,  2042,   896,    11,  8854,     6,    38,   168,    38,\n",
      "          5815,   112,  4999, 23586,    15,     7,    38,     3,     9,  1144,\n",
      "            13,     8,  6923,  3815,     5,    37,   296,    31,     7,  4016,\n",
      "         11508,    49,     6,    38,   168,    38,     3,     9, 11766, 16600,\n",
      "             7,   348,    11, 22526,  2377,     6,  1862, 25810, 17274,     7,\n",
      "             3,     9,   620,    13,  7873,     3,  6770,     7,    41,    77,\n",
      "          4092,   648,     6,    79,    33,     3,  4822,    38,    96, 17434,\n",
      "            17,    63,     3,  6770,     7,   121,  6306,   357,   908,    61,\n",
      "            28,   796,   534,  3621,     6,   224,    38, 13470,     6, 19758,\n",
      "            18,    17, 15437,     6, 31031,    53,  7970,     6,  4923,     3,\n",
      "         16677,     9,   221,     6, 11482,  1807,     6,    11,   237,     3,\n",
      "           157,   651, 11632,   155,    15,     3,  6770,     7,    21,   169,\n",
      "            16,     3,     9,   620,    13,   534,  4147,     5,  1862, 25810,\n",
      "          2994,  8107,  1269,    16,   112,   778,   203,     6,  2852,     8,\n",
      "          1189,  1451,    13,  1537,  9259,     6,    38,   168,    38,   578,\n",
      "         14521,  3179,     7,    16,   119,  7967,     7,     5,     3, 11714,\n",
      "           112,   166,  6786,    18, 16443,   203,     6,   983,     6,     8,\n",
      "          1848,   470,  2994,  2123,  9897,     5,    86,     8,  1480,  8754,\n",
      "             7,     6,  4346,  7272,    63,   411,    31,   567,    15,   173,\n",
      "             6,  3555,    57,     8,  1848,    31,     7,  8417,  3176, 22819,\n",
      "            57,  1484,   138, 14274,     6,  3260,    12,    43,   376,  2615,\n",
      "           112, 13462,     6,  1517,   376,     8,   258,    18,   202,  1495,\n",
      "          1075,    13,     3,     9,  2815, 10684,  8396,     7,     9,   588,\n",
      "            21,     8,   464,   853,    11,     8,     3, 30801,     5,    86,\n",
      "          7434,     6,     3,    88,    47,     3, 13804,    28,     3,     9,\n",
      "            72,   973,    11,   455,    18,  9442,   160,    32,     6,  1862,\n",
      "          9144,  2947,     6,    16,     3,     9,  1591,    18, 18087,     6,\n",
      "           569,   120, 13381,  7967,   484,   939,     5,  6306,   519,   908,\n",
      "          1541,   258,     6,     3,    88,    65,   118,  1012,   859,  7967,\n",
      "           484,  2675,    11,   167,  5943,    43,  1026,    46,  4150,     6,\n",
      "          3542, 17132,  1295,    12,     8,  1848,     5, 15865,  5286,    47,\n",
      "          4792,   326,    16,     8,  5541,     7,    11,  5821,    57,     3,\n",
      "             9,   126,  1848,     6, 15865,    31,     7,   520,     3, 22329,\n",
      "         12833,    15,     5,     3, 22329,     6,   983,     6,  9193,     3,\n",
      "             9,   705,  1012,  1848,     6,    11,     8,   926, 15865,  5286,\n",
      "          1848,    47,     3,    60,  3042,    52,  7633,    16,     8,  4402,\n",
      "            96,  5991,    23,   624,   121,   733,   747,     6,    57,  4346,\n",
      "          8595,  3931,     5,    86,     8,  2766,     7,     6,     8,  1848,\n",
      "            65,   118,  4510,    16,  4038,   733,  6972,     3,  7388,    30,\n",
      "          1862, 25810,    11,  1589,  1072,  1208,     6,   224,    38,     8,\n",
      "          5795,   605,    37,  1862, 25810,    87, 20096,  1072,  1208,  9057,\n",
      "            11,     8,   306,    18, 18816,  6923,  3815,    10,   205,   651,\n",
      "            21,  6923,   733,   747,     6,  1884,    12,     8,  1848,    31,\n",
      "             7,     3,    60, 27493,  5815,   167,    13,  5795,    31,     7,\n",
      "          2605,    16,  8088,  1862, 25810,    47,    59,  7513,     3,     9,\n",
      "           168,    18,  5661,  1848,  1067,    13,  7967,   484,  1819,  5012,\n",
      "            10,   216,   141,  4283,    16,     3,     9,   712,  5640,    13,\n",
      "             8, 16822,   939,  2011,  9779,    16, 17107,     5,    86,     8,\n",
      "          2766,     7,     6,     8,  1848,  4283,    16,     3,     9,   381,\n",
      "            13,  5795,  4390,  2605,     6,   379,     8, 16822,   939,  6923,\n",
      "          3815, 24813,     6,  5209,  6923,     6,    37, 20644,    11, 20644,\n",
      "            10,    37,  3497,   162,    11,     8,  8166,    26,     6,    11,\n",
      "           633,  5795, 20713,     3, 19209,   920,  8465, 10743,     7,     5,\n",
      "            86,   619,  1041,     6,     3,    88,  4283,    16,     8,   939,\n",
      "          4872,  1420,     6,  1944,    57,  7556, 12446, 10498,  1306,     6,\n",
      "            11,  1632,     3,     9,  2583,  4061,  1144,     5,    86,  1673,\n",
      "             6,     8,   619,  1041,   939, 25810,    20, 29261,    30,    37,\n",
      "             3, 18105,     6,    16,    84,     8,  2233,  1848,    47,     3,\n",
      "         27486,    57,  7872,   736,  3820,     6,    11,     3, 14138,   633,\n",
      "          5404,    18,  1647,   939,     6,  2852,     8,  1684,   500,    21,\n",
      "             3,     9,  2471,  4390,  7884,   718,     8, 25810,  7583,     5,\n",
      "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generating a Summary\n",
    "We pass the tokenized input to `model.generate()` to create a summary. \n",
    "\n",
    "### **Decoding Strategies Used:**\n",
    "- `max_length=200` → Limits the length of the summary.\n",
    "- `num_beams=20` → Uses beam search for better-quality outputs.\n",
    "- `no_repeat_ngram_size=2` → Prevents repeated phrases.\n",
    "- `early_stopping=True` → Stops generation when output seems complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green Arrow is an archer who uses his skills to fight crime in his home cities. he was paired with a more law and order-oriented hero, Green Lantern, in 1970s, resurrected in the 2001 \"Quiver\" storyline. in 2012, the live action series Arrow debuted on The CW.\n"
     ]
    }
   ],
   "source": [
    "output_ids = model.generate(\n",
    "    input_ids = input_tokens[\"input_ids\"],\n",
    "    attention_mask = input_tokens[\"attention_mask\"],\n",
    "    max_length = 200,\n",
    "    num_beams = 20,\n",
    "    early_stopping = True,\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "\n",
    "\n",
    "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
