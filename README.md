# ğŸ§  T5 Text Summarization â€“ Mini Project

This project demonstrates how to use the T5 Transformer model for **text summarization** using Hugging Face ğŸ¤— Transformers. It includes hands-on steps for running inference, fine-tuning on custom data, and evaluating the fine-tuned model.

---

## ğŸ” Objective

- Understand **sequence-to-sequence** (text-to-text) modeling using T5
- Learn how to tokenize, generate summaries, and decode outputs
- Fine-tune a pretrained model and visualize training loss
- Run inference on unseen text using the fine-tuned model

---

## ğŸ§± Folder Structure

t5_text_summarization/ â”‚ â”œâ”€â”€ 01_intro_and_setup.ipynb â† Concept + basic testing â”œâ”€â”€ 02_tokenization_and_basics.ipynbâ† Tokenizing & decoding walkthrough â”œâ”€â”€ 03_inference_baseline.ipynb â† Summarization using pretrained model â”œâ”€â”€ 04_fine_tuning.ipynb â† Training loop, loss logging, model save â”œâ”€â”€ 05_evaluation.ipynb â† Load trained model & run new inference â”‚ â”œâ”€â”€ results/ â”‚ â””â”€â”€ t5_finetuned/ â† Fine-tuned model & tokenizer

yaml
Copy
Edit

---

## âš™ï¸ How to Run

1. Clone the repo
2. Install required packages:
   ```bash
   pip install torch transformers datasets matplotlib
Run notebooks in order:

01_intro_and_setup.ipynb â†’ 05_evaluation.ipynb

âœ¨ Example Output
Input:

summarize: Artificial Intelligence is transforming healthcare...

Generated Summary:

AI-powered tools can analyze vast medical data, assist in diagnostics, and recommend personalized treatments.

ğŸ“š What I Learned
How T5 models handle summarization as a text-to-text problem

How tokenization works and why decoding is needed

How to fine-tune a Transformer on small datasets

Importance of beam search, padding, and decoding strategies

ğŸ“Œ Next Steps
Try other summarization datasets (e.g., XSum, CNN-DailyMail)

Experiment with longer models like t5-base or t5-large

Add Streamlit or Gradio frontend for live summarization